{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "409552e8-30ea-4c9e-87e7-2fa14cc210d7",
   "metadata": {},
   "source": [
    "# Сбор данных с хедхантера. \n",
    "\n",
    "Тестирование и проверка сбора данных. Итоговая версия - скриптом, по расписанию."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e8c61b3-baab-4601-b480-b37c1fd967b2",
   "metadata": {},
   "source": [
    "## Ссылки на доки по API\n",
    "* [hh_research](https://github.com/hukenovs/hh_research)\n",
    "* [Хабр](https://habr.com/ru/post/666062/)\n",
    "* [HeadHunter API](https://github.com/hhru/api)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac5bcd2-13fa-4d52-aff0-f20b05fa04b3",
   "metadata": {},
   "source": [
    "## TODO:\n",
    "* Add automation down to the date to date periods\n",
    "* Refactor back into the scripts\n",
    "* Dynamic fields download (maybe download&save a raw JSON object and process it later?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4507a27d-620f-46a6-8ec6-6af0190b5e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# modules import\n",
    "import hashlib\n",
    "import os\n",
    "import pickle\n",
    "import re\n",
    "\n",
    "from datetime import timedelta as td, datetime, date\n",
    "\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from typing import Dict, List, Optional\n",
    "from urllib.parse import urlencode\n",
    "\n",
    "import requests\n",
    "from requests.adapters import HTTPAdapter\n",
    "from requests.packages.urllib3.util.retry import Retry\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fec0f27-3fd2-411f-835f-7c02f87195c2",
   "metadata": {},
   "source": [
    "## Возвращаемые столбцы и соответствие в API **vacancies**:\n",
    "\n",
    "| Возвращаемый столбец       | Путь в API                                              |\n",
    "| -------------------------- | ------------------------------------------------------- |\n",
    "| vacancy_id | id |\n",
    "| vacancy_name | name |\n",
    "| area_name | area\\name |\n",
    "| employer_id | employer\\id |\n",
    "| employer_name | employer\\name |\n",
    "| employer_alternate_url | employer\\alternate_url |\n",
    "| salary_from | salary\\from |\n",
    "| salary_to | salary\\to |\n",
    "| salary_currency | salary\\currency |\n",
    "| experience_name | experience\\name |\n",
    "| schedule_name | schedule\\name |\n",
    "| employment_name | employment\\name |\n",
    "| key_skills | key_skills - список key_skills\\id\\name |\n",
    "| specializations | specializations\\id\\name |\n",
    "| professional_roles | professional_roles\\id\\name |\n",
    "| published_at | published_at |\n",
    "| created_at | created_at |\n",
    "| initial_created_at | initial_created_at |\n",
    "| alternate_url | alternate_url |\n",
    "| description | description |\n",
    "| responses | counters\\responses |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d9c551cf-4432-47dd-9252-6f64a6080f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class constants\n",
    "# API base url address\n",
    "__API_BASE_URL = \"https://api.hh.ru/vacancies/\"\n",
    "\n",
    "# cache folder \n",
    "CACHE_DIR = \"../cache/\"\n",
    "\n",
    "# column names for the returned table\n",
    "__DICT_KEYS = (\n",
    "    \"vacancy_id\",\n",
    "    \"vacancy_name\",\n",
    "    \"area_name\",\n",
    "    \"employer_id\",\n",
    "    \"employer_name\",\n",
    "    \"employer_alternate_url\",\n",
    "    \"salary_from\",\n",
    "    \"salary_to\",\n",
    "    \"salary_currency\",\n",
    "    \"experience_name\",\n",
    "    \"schedule_name\",\n",
    "    \"employment_name\",\n",
    "    \"key_skills\",\n",
    "    \"specializations\",\n",
    "    \"professional_roles\",\n",
    "    \"published_at\",\n",
    "    \"created_at\",\n",
    "    \"initial_created_at\",\n",
    "    \"alternate_url\",\n",
    "    \"description\",\n",
    "    \"responses\",\n",
    "    \"retrieve_date\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f25c2e2-569d-4b50-a9d1-3f25b85a56a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_tags(html_text: str) -> str:\n",
    "    \"\"\"Remove HTML tags from the string\n",
    "    Parameters\n",
    "    ----------\n",
    "    html_text: str\n",
    "        Input string with tags\n",
    "    Returns\n",
    "    -------\n",
    "    result: string\n",
    "        Clean text without HTML tags\n",
    "    \"\"\"\n",
    "    pattern = re.compile(\"<.*?>\")\n",
    "    return re.sub(pattern, \"\", html_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ad89eae7-27ca-48a4-878a-263e24fd7f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vacancy(vacancy_id: str, resps: int):\n",
    "    # Getting vacancy data from URL\n",
    "    # TODO - dynamic field processing?\n",
    "    # Return fields (in order):\n",
    "    # vacancy_id\n",
    "    # [\"name\"]\n",
    "    # [\"area\"][\"name\"]\n",
    "    # [\"employer\"][\"id\"]\n",
    "    # [\"employer\"][\"name\"]\n",
    "    # [\"employer\"][\"alternate_url\"]\n",
    "    # [\"salary\"][\"from\"]\n",
    "    # [\"salary\"][\"to\"]\n",
    "    # [\"salary\"][\"currency\"]\n",
    "    # [\"experience\"][\"name\"]\n",
    "    # [\"schedule\"][\"name\"]\n",
    "    # [\"employment\"][\"name\"]\n",
    "    # list of [\"key_skills\"][\"id\"][\"name\"]\n",
    "    # list of [\"specializations\"][\"id\"][\"name\"]\n",
    "    # list of [\"professional_roles\"][\"id\"][\"name\"]\n",
    "    # [\"published_at\"]\n",
    "    # [\"created_at\"]\n",
    "    # [\"initial_created_at\"]\n",
    "    # [\"alternate_url\"]\n",
    "    # [\"description\"]\n",
    "    # \"responses\" - taken as an argument\n",
    "    # \"retrieve_date\" - current date\n",
    "    url = f\"{__API_BASE_URL}{vacancy_id}\"\n",
    "    try:\n",
    "        vacancy = requests.api.get(url).json()\n",
    "    except:\n",
    "        return (\n",
    "            vacancy_id,\n",
    "            None,\n",
    "            None,\n",
    "            None,\n",
    "            None,\n",
    "            None,\n",
    "            None,\n",
    "            None,\n",
    "            None,\n",
    "            None,\n",
    "            None,\n",
    "            None,\n",
    "            None,\n",
    "            None,\n",
    "            None,\n",
    "            None,\n",
    "            None,\n",
    "            None,\n",
    "            None,\n",
    "            None,\n",
    "            None,\n",
    "            None,\n",
    "        )\n",
    "    # Checking salary as it can be null\n",
    "    salary = vacancy.get(\"salary\")\n",
    "    salary_data = {\"from\": None, \"to\": None, \"currency\": None}\n",
    "    if salary is not None:\n",
    "        salary_data[\"from\"] = salary[\"from\"]\n",
    "        salary_data[\"to\"] = salary[\"to\"]\n",
    "        salary_data[\"currency\"] = salary[\"currency\"]\n",
    "    # Checking employer for None\n",
    "    employer = vacancy.get(\"employer\")\n",
    "    employer_data = {\"id\": None, \"name\": None, \"alternate_url\": None}\n",
    "    if employer is not None:\n",
    "        employer_data[\"id\"] = employer.get(\"id\")\n",
    "        employer_data[\"name\"] = employer.get(\"name\")\n",
    "        employer_data[\"alternate_url\"] = employer.get(\"alternate_url\")\n",
    "    key_skills = vacancy.get(\"key_skills\")\n",
    "    if key_skills is not None:\n",
    "        key_skills_data = [item[\"name\"] for item in key_skills]\n",
    "    else:\n",
    "        key_skills_data = []\n",
    "    specializations = vacancy.get(\"specializations\")\n",
    "    if specializations is not None:\n",
    "        specializations_data = [item[\"name\"] for item in specializations]\n",
    "    else:\n",
    "        specializations_data = []\n",
    "    professional_roles = vacancy.get(\"professional_roles\")\n",
    "    if professional_roles is not None:\n",
    "        professional_roles_data = [item[\"name\"] for item in professional_roles]\n",
    "    else:\n",
    "        professional_roles_data = []\n",
    "    # Create pages tuple\n",
    "    return (\n",
    "        vacancy_id,\n",
    "        vacancy.get(\"name\"),\n",
    "        vacancy.get('area', {}).get('name'),\n",
    "        employer_data[\"id\"],\n",
    "        employer_data[\"name\"],\n",
    "        employer_data[\"alternate_url\"],\n",
    "        salary_data[\"from\"],\n",
    "        salary_data[\"to\"],\n",
    "        salary_data[\"currency\"],\n",
    "        vacancy.get('experience', {}).get('name'),\n",
    "        vacancy.get('schedule', {}).get('name'),\n",
    "        vacancy.get('employment', {}).get('name'),\n",
    "        key_skills_data,\n",
    "        specializations_data,\n",
    "        professional_roles_data,\n",
    "        vacancy.get(\"published_at\"),\n",
    "        vacancy.get(\"created_at\"),\n",
    "        vacancy.get(\"initial_created_at\"),\n",
    "        vacancy.get(\"alternate_url\"),\n",
    "        clean_tags(str(vacancy.get(\"description\") or \"\")),\n",
    "        resps,\n",
    "        date.today().isoformat()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cfb9e4ef-6eb1-4f34-ad5a-4a8755701600",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_vacancies(query: Optional[Dict],\n",
    "                      existing_ids: Optional[List],\n",
    "                      refresh: bool = False,\n",
    "                      responses: bool = False,\n",
    "                      progress_info: bool = True,\n",
    "                      max_workers: int = 1) -> (Dict, int):\n",
    "    \"\"\"Parse vacancy JSON: get vacancy name, salary, experience etc.\n",
    "    Parameters\n",
    "    ----------\n",
    "    query : dict\n",
    "        Search query params for GET requests.\n",
    "    existing_ids : list\n",
    "        List with existing vacancy ids (taken either for the same date beforehand or the whole dataset)\n",
    "    refresh :  bool\n",
    "        Refresh cached data\n",
    "    responses : bool\n",
    "        Whether to collect the number of vacancy responses or not\n",
    "    max_workers :  int\n",
    "        Number of workers for threading.\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        Dict of useful data from vacancies\n",
    "    int\n",
    "        API request counter\n",
    "    \"\"\"\n",
    "\n",
    "    # Get cached data if exists...\n",
    "    cache_name: str = urlencode(query)\n",
    "    cache_hash = hashlib.md5(cache_name.encode()).hexdigest()\n",
    "    cache_file = os.path.join(CACHE_DIR, cache_hash)\n",
    "    result = {}\n",
    "    api_counter = 0\n",
    "    \n",
    "    try:\n",
    "        if not refresh:\n",
    "            if progress_info:\n",
    "                print(f\"[INFO]: Geting results from cache! Enable refresh option to update results.\")\n",
    "            return pickle.load(open(cache_file, \"rb\"))\n",
    "    except (FileNotFoundError, pickle.UnpicklingError):\n",
    "        pass\n",
    "    \n",
    "    if existing_ids is None:\n",
    "        existing_ids = []   \n",
    "   \n",
    "    if responses:\n",
    "        query['responses_count_enabled'] = True\n",
    "\n",
    "    # Customize HTTPAdapter and Retry Strategy\n",
    "    retry_strategy = Retry(\n",
    "        total=10,\n",
    "        status_forcelist=[413, 429, 503],\n",
    "        allowed_methods=[\"HEAD\", \"GET\", \"PUT\", \"DELETE\", \"OPTIONS\", \"TRACE\"],\n",
    "        backoff_factor=1\n",
    "    )\n",
    "    adapter = HTTPAdapter(max_retries=retry_strategy)\n",
    "    http = requests.Session()\n",
    "    http.mount(\"https://\", adapter)\n",
    "    http.mount(\"http://\", adapter)\n",
    "        \n",
    "    target_url = __API_BASE_URL + \"?\" + urlencode(query)\n",
    "    num_pages = http.get(target_url).json().get(\"pages\")\n",
    "    if num_pages is None:\n",
    "        return result, 1\n",
    "    \n",
    "    # Collect vacancy IDs...\n",
    "    ids = []\n",
    "    resps = []\n",
    "    \n",
    "    for idx in range(num_pages + 1):\n",
    "        response = requests.get(target_url, {\"page\": idx})\n",
    "        api_counter +=1\n",
    "        data = response.json()\n",
    "        if \"items\" not in data:\n",
    "            break\n",
    "        ids.extend(x[\"id\"] for x in data[\"items\"])\n",
    "        resps.extend(x.get('counters', {}).get('responses') for x in data[\"items\"])\n",
    "\n",
    "    ids = list(set(ids) - set(existing_ids))\n",
    "    \n",
    "    # Collect vacancies...\n",
    "    jobs_list = []\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        if progress_info:\n",
    "            for vacancy in tqdm(\n",
    "                executor.map(get_vacancy, ids, resps),\n",
    "                desc=\"Get data via HH API\",\n",
    "                ncols=100,\n",
    "                total=len(ids),\n",
    "            ):\n",
    "                jobs_list.append(vacancy)\n",
    "                api_counter+=1\n",
    "        else:\n",
    "            for vacancy in executor.map(get_vacancy, ids, resps):\n",
    "                jobs_list.append(vacancy)\n",
    "                api_counter+=1\n",
    "                \n",
    "    unzipped_list = list(zip(*jobs_list))\n",
    "\n",
    "    if len(unzipped_list) > 0:\n",
    "        for idx, key in enumerate(__DICT_KEYS):\n",
    "            result[key] = unzipped_list[idx]\n",
    "        pickle.dump(result, open(cache_file, \"wb\"))\n",
    "        \n",
    "    return result, api_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ac33448f-503c-463a-8e55-8265a8096f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_responses(\n",
    "                     query: Optional[Dict],\n",
    "                     max_workers: int = 1) -> (pd.DataFrame, int):\n",
    "    \"\"\"Add response numbers to the separate dataframe.\n",
    "    Dataframe structure: vacancy_id, retrieve_date, responses\n",
    "    Parameters\n",
    "    ----------\n",
    "    query : dict\n",
    "        Search query params for GET requests.\n",
    "    max_workers :  int\n",
    "        Number of workers for threading.\n",
    "    Returns\n",
    "    -------\n",
    "    dataframe\n",
    "        dataframe with the responses\n",
    "    int\n",
    "        API request counter\n",
    "    \"\"\"\n",
    "    api_counter = 0\n",
    "    \n",
    "    # Customize HTTPAdapter and Retry Strategy\n",
    "    retry_strategy = Retry(\n",
    "        total=10,\n",
    "        status_forcelist=[413, 429, 503],\n",
    "        allowed_methods=[\"HEAD\", \"GET\", \"PUT\", \"DELETE\", \"OPTIONS\", \"TRACE\"],\n",
    "        backoff_factor=1\n",
    "    )\n",
    "    adapter = HTTPAdapter(max_retries=retry_strategy)\n",
    "    http = requests.Session()\n",
    "    http.mount(\"https://\", adapter)\n",
    "    http.mount(\"http://\", adapter)\n",
    "        \n",
    "    target_url = __API_BASE_URL + \"?\" + urlencode(query)\n",
    "    num_pages = http.get(target_url).json().get(\"pages\")\n",
    "    if num_pages is None:\n",
    "        return result, 1\n",
    "    \n",
    "    # Collect vacancy IDs...\n",
    "    ids = []\n",
    "    retrieve_dates = []\n",
    "    resps = []\n",
    "    \n",
    "    for idx in range(num_pages + 1):\n",
    "        response = requests.get(target_url, {\"page\": idx})\n",
    "        api_counter +=1\n",
    "        data = response.json()\n",
    "        if \"items\" not in data:\n",
    "            break\n",
    "        for item in data[\"items\"]:\n",
    "            ids.append(item[\"id\"])\n",
    "            resps.append(round(item.get('counters', {}).get('responses')))\n",
    "            retrieve_dates.append(date.today().isoformat())\n",
    "    result = pd.DataFrame(data={'vacancy_id': ids,'retrieve_date': retrieve_dates, 'responses': resps})\n",
    "    return result, api_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "40ca1c0f-eddd-43e5-aeed-6a40667938d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Старые ключевые слова\n",
    "query_texts_old = [\n",
    "    \"Аналитик\",\n",
    "    \"Инженер данных\",\n",
    "    \"Data Scientist\",\n",
    "]\n",
    "# Текущие ключевые слова\n",
    "query_texts = [\n",
    "    \"Аналитик данных\",\n",
    "    \"Системный аналитик\",\n",
    "    \"Бизнес аналитик\",\n",
    "    \"Продуктовый аналитик\",\n",
    "    \"Веб-аналитик\",\n",
    "    \"Инженер данных\",\n",
    "    \"Data Engineer\",\n",
    "    \"Data Scientist\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "23a94de7-de9f-48ec-8c4d-c309a2a146a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Текущие ключевые слова\n",
    "query_texts2 = [\n",
    "    \"Аналитик данных\",\n",
    "    \"Системный аналитик\",\n",
    "    \"Бизнес аналитик\",\n",
    "    \"Продуктовый аналитик\",\n",
    "    \"Marketing Analyst\",\n",
    "    \"Веб-аналитик\",\n",
    "    \"Аналитик BI\",\n",
    "    \"Младший аналитик\",\n",
    "    \"Руководитель отдела аналитики\",\n",
    "    \"Аналитик баз данных\",\n",
    "    \n",
    "    \"Инженер баз данных\",\n",
    "    \n",
    "    \"Data Engineer\",\n",
    "    \"Data Scientist\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cc8c39ed-c825-4d03-ac30-0f591b42867a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ключевые слова из дашборда revealthedata\n",
    "texts_rsd = [\n",
    "    \"Бизнес аналитик\",\n",
    "    \"Аналитик данных\",\n",
    "    \"Marketing Analyst\",\n",
    "    \"Продуктовый аналитик\",\n",
    "    \"Аналитик BI\",\n",
    "    \"Младший аналитик\",\n",
    "    \"Руководитель отдела аналитики\",\n",
    "    \"Аналитик баз данных\",\n",
    "    \n",
    "    \"Разработчик BI\",\n",
    "    \n",
    "    \"Инженер баз данных\",\n",
    "    \n",
    "    \"Data scientist\",\n",
    "    \"Руководитель направления предикативной аналитики\",\n",
    "    \"Руководитель data science\",\n",
    "    \"Data Science Director\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "4aa18ca4-dabf-4883-be30-206156b77155",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_responses(query_texts, query_date):\n",
    "    \"\"\"\n",
    "    Update responses for the vacancies from the certain date\n",
    "    Data is saved to f\"../data/download/responses/responses.csv\" as CSV files.\n",
    "    Parameters\n",
    "    ----------\n",
    "    query_texts : list\n",
    "        List of strings with the search query request\n",
    "    query_date : string\n",
    "        Date string in ISO 8601 format (YYYY-MM-DD)\n",
    "    Returns\n",
    "    -------\n",
    "    bool\n",
    "        True if all the data retrieved, False if API limit reached\n",
    "    \"\"\"\n",
    "    # a counter for 3k/hour API limit\n",
    "    api_hourly_limit = 3000\n",
    "    api_counter = 0\n",
    "    # distribute vacancies by hour to avoid the 2k API limit\n",
    "    timelist = pd.to_datetime(pd.date_range(start = query_date,\n",
    "                             periods=25,\n",
    "                             freq = \"H\")).strftime('%Y-%m-%dT%H:%M:%S').to_list()\n",
    "    try:\n",
    "        responses_df = pd.read_csv(f\"../data/download/responses/responses.csv\", dtype={\n",
    "            'vacancy_id': str,\n",
    "        })\n",
    "    except:\n",
    "        responses_df = None\n",
    "    responses_data = []\n",
    "    # iterate through texts and every hour in the day \n",
    "    for query_id in range (0, len(query_texts)):\n",
    "        print(f\"Downloading for query '{query_texts[query_id]}' for {query_date}\")          \n",
    "        for i in range(1, 25):\n",
    "            temp_data = get_responses(\n",
    "                    query={\"text\": query_texts[query_id],\n",
    "                           \"per_page\": 50,\n",
    "                           \"date_from\": timelist[i-1],\n",
    "                           \"date_to\": timelist[i],\n",
    "                           'responses_count_enabled': True,\n",
    "                          },\n",
    "                )\n",
    "            responses_data.append(pd.DataFrame(temp_data[0]))\n",
    "            api_counter += temp_data[1]\n",
    "            if api_counter >= api_hourly_limit:\n",
    "                break\n",
    "        if api_counter >= api_hourly_limit:\n",
    "            print(f\"API download limit reached, downloaded {api_counter} vacancies for {query_date}\")\n",
    "            responses_df_new = pd.concat(responses_data)\n",
    "            responses_df = pd.concat([responses_df, responses_df_new])\n",
    "            responses_df = responses_df.drop_duplicates(subset=['vacancy_id', 'retrieve_date'], keep='last') \n",
    "            responses_df.to_csv(f\"../data/download/responses/responses.csv\",index=False)\n",
    "            return False\n",
    "    print(f\"Downloaded all {api_counter} responses for {query_date}\")\n",
    "    \n",
    "    # write the dataframe\n",
    "    responses_df_new = pd.concat(responses_data)\n",
    "    responses_df = pd.concat([responses_df, responses_df_new])\n",
    "    responses_df = responses_df.drop_duplicates(subset=['vacancy_id', 'retrieve_date'], keep='last') \n",
    "    responses_df.to_csv(f\"../data/download/responses/responses.csv\",index=False)      \n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "93d257b4-d19b-48ed-aa52-30c0c224a6c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_queries(query_texts, query_date, refresh = True, progress_info = True):\n",
    "    \"\"\"\n",
    "    Retrieve data for the list of queries for the certain date.    \n",
    "    Data is saved to f\"../data/download/{query_date}_{query_texts[query_id]}.csv\" as CSV files.\n",
    "    Parameters\n",
    "    ----------\n",
    "    query_texts : list\n",
    "        List of strings with the search query request\n",
    "    query_date : string\n",
    "        Date string in ISO 8601 format (YYYY-MM-DD)\n",
    "    Returns\n",
    "    -------\n",
    "    bool\n",
    "        True if all the data retrieved, False if API limit reached\n",
    "    \"\"\"\n",
    "    # a counter for 3k/hour API limit\n",
    "    api_hourly_limit = 3000\n",
    "    api_counter = 0\n",
    "    dropped_counter = 0\n",
    "    # distribute vacancies by hour to avoid the 2k API limit\n",
    "    timelist = pd.to_datetime(pd.date_range(start = query_date,\n",
    "                             periods=25,\n",
    "                             freq = \"H\")).strftime('%Y-%m-%dT%H:%M:%S').to_list()\n",
    "    # iterate through texts and every hour of the day \n",
    "    for query_id in range (0, len(query_texts)):\n",
    "        print(f\"Downloading for query '{query_texts[query_id]}' for {query_date}\")\n",
    "        try:\n",
    "            vacancies_df = pd.read_csv(f\"../data/download/{query_date}_{query_texts[query_id]}.csv\", dtype={\n",
    "                'vacancy_id': str,\n",
    "                'employer_id': str,\n",
    "            })\n",
    "        except:\n",
    "            vacancies_df = None\n",
    "        if (vacancies_df is not None):\n",
    "            if not refresh:\n",
    "                continue\n",
    "            existing_ids = vacancies_df['vacancy_id'].to_list()\n",
    "        else:\n",
    "            existing_ids = []\n",
    "        vacancies_data = []\n",
    "        for i in range(1, 25):\n",
    "            temp_data = collect_vacancies(\n",
    "                    query={\"text\": query_texts[query_id],\n",
    "                           \"per_page\": 50,\n",
    "                           \"date_from\": timelist[i-1],\n",
    "                           \"date_to\": timelist[i],\n",
    "                          },\n",
    "                    existing_ids=existing_ids,\n",
    "                    responses=True,\n",
    "                    refresh=refresh,\n",
    "                    progress_info=progress_info,\n",
    "                )\n",
    "            vacancies_data.append(pd.DataFrame(temp_data[0]))\n",
    "            api_counter += temp_data[1]\n",
    "            if api_counter >= api_hourly_limit:\n",
    "                break\n",
    "        # combine daily data into the dataframe, remove duplicates and ave it\n",
    "        vacancies_df_new = pd.concat(vacancies_data)\n",
    "        vacancies_df = pd.concat([vacancies_df, vacancies_df_new])\n",
    "        if vacancies_df.shape[0] == 0:\n",
    "            continue\n",
    "        dropped_counter += vacancies_df['vacancy_name'].isnull().sum()\n",
    "        vacancies_df = vacancies_df[vacancies_df['vacancy_name'].notnull()]\n",
    "        vacancies_df = vacancies_df.drop_duplicates(subset='vacancy_id')\n",
    "        vacancies_df['query'] = query_texts[query_id]\n",
    "        vacancies_df.to_csv(f\"../data/download/{query_date}_{query_texts[query_id]}.csv\",index=False)\n",
    "        if api_counter >= api_hourly_limit:\n",
    "            print(f\"API download limit reached, downloaded {api_counter} vacancies for {query_date}\")\n",
    "            return False\n",
    "    print(f\"Downloaded all {api_counter} vacancies for {query_date}\")\n",
    "    if dropped_counter > 0:\n",
    "        print(f\"Removed nulls: {dropped_counter}\")\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3c8ec820-9506-4238-8c3d-0dd2145f35db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve the data for the last week (not including today)\n",
    "# stop if the api download limit reached\n",
    "# text query is taken from query_texts\n",
    "api_status = True\n",
    "today = date.today()\n",
    "#for i in range(7, 0, -1):\n",
    "#    day = today - td(days = i)\n",
    "#    query_date = day.isoformat()\n",
    "#    api_status = retrieve_queries(query_texts, query_date, False, False)\n",
    "#    if not api_status:\n",
    "#        print(\"API limit reached, wait for an hour\")\n",
    "#        break\n",
    "#if api_status:\n",
    "#    print(\"Data for the last week downloaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "636e0c46-2e9f-4ed3-b7ad-03618c98a453",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading for query 'Аналитик данных' for 2022-09-22\n",
      "Downloading for query 'Системный аналитик' for 2022-09-22\n",
      "Downloading for query 'Бизнес аналитик' for 2022-09-22\n",
      "Downloading for query 'Продуктовый аналитик' for 2022-09-22\n",
      "Downloading for query 'Marketing Analyst' for 2022-09-22\n",
      "Downloading for query 'Веб-аналитик' for 2022-09-22\n",
      "Downloading for query 'Аналитик BI' for 2022-09-22\n",
      "Downloading for query 'Младший аналитик' for 2022-09-22\n",
      "Downloading for query 'Руководитель отдела аналитики' for 2022-09-22\n",
      "Downloading for query 'Аналитик баз данных' for 2022-09-22\n",
      "Downloading for query 'Инженер баз данных' for 2022-09-22\n",
      "Downloading for query 'Data Engineer' for 2022-09-22\n",
      "Downloading for query 'Data Scientist' for 2022-09-22\n",
      "Downloaded all 624 responses for 2022-09-22\n",
      "Downloading for query 'Аналитик данных' for 2022-09-15\n",
      "Downloading for query 'Системный аналитик' for 2022-09-15\n",
      "Downloading for query 'Бизнес аналитик' for 2022-09-15\n",
      "Downloading for query 'Продуктовый аналитик' for 2022-09-15\n",
      "Downloading for query 'Marketing Analyst' for 2022-09-15\n",
      "Downloading for query 'Веб-аналитик' for 2022-09-15\n",
      "Downloading for query 'Аналитик BI' for 2022-09-15\n",
      "Downloading for query 'Младший аналитик' for 2022-09-15\n",
      "Downloading for query 'Руководитель отдела аналитики' for 2022-09-15\n",
      "Downloading for query 'Аналитик баз данных' for 2022-09-15\n",
      "Downloading for query 'Инженер баз данных' for 2022-09-15\n",
      "Downloading for query 'Data Engineer' for 2022-09-15\n",
      "Downloading for query 'Data Scientist' for 2022-09-15\n",
      "Downloaded all 624 responses for 2022-09-15\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "update_responses(query_texts2, (date.today() - td(days = 8)).isoformat())\n",
    "update_responses(query_texts2, (date.today() - td(days = 15)).isoformat())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "9acc0925-6068-42cd-ae1d-bfcc6c9993a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading for query 'Аналитик данных' for 2022-09-29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Get data via HH API: 0it [00:00, ?it/s]\n",
      "Get data via HH API: 0it [00:00, ?it/s]\n",
      "Get data via HH API: 0it [00:00, ?it/s]\n",
      "Get data via HH API: 0it [00:00, ?it/s]\n",
      "Get data via HH API: 0it [00:00, ?it/s]\n",
      "Get data via HH API: 0it [00:00, ?it/s]\n",
      "Get data via HH API: 0it [00:00, ?it/s]\n",
      "Get data via HH API: 0it [00:00, ?it/s]\n",
      "Get data via HH API: 0it [00:00, ?it/s]\n",
      "Get data via HH API: 0it [00:00, ?it/s]\n",
      "Get data via HH API: 0it [00:00, ?it/s]\n",
      "Get data via HH API: 0it [00:00, ?it/s]\n",
      "Get data via HH API: 0it [00:00, ?it/s]\n",
      "Get data via HH API: 0it [00:00, ?it/s]\n",
      "Get data via HH API: 0it [00:00, ?it/s]\n",
      "Get data via HH API: 0it [00:00, ?it/s]\n",
      "Get data via HH API: 0it [00:00, ?it/s]\n",
      "Get data via HH API: 0it [00:00, ?it/s]\n",
      "Get data via HH API: 0it [00:00, ?it/s]\n",
      "Get data via HH API: 0it [00:00, ?it/s]\n",
      "Get data via HH API: 0it [00:00, ?it/s]\n",
      "Get data via HH API: 0it [00:00, ?it/s]\n",
      "Get data via HH API: 0it [00:00, ?it/s]\n",
      "Get data via HH API: 0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading for query 'Системный аналитик' for 2022-09-29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Get data via HH API: 0it [00:00, ?it/s]\n",
      "Get data via HH API: 0it [00:00, ?it/s]\n",
      "Get data via HH API: 0it [00:00, ?it/s]\n",
      "Get data via HH API: 0it [00:00, ?it/s]\n",
      "Get data via HH API: 0it [00:00, ?it/s]\n",
      "Get data via HH API: 0it [00:00, ?it/s]\n",
      "Get data via HH API: 0it [00:00, ?it/s]\n",
      "Get data via HH API: 0it [00:00, ?it/s]\n",
      "Get data via HH API: 0it [00:00, ?it/s]\n",
      "Get data via HH API: 0it [00:00, ?it/s]\n",
      "Get data via HH API: 0it [00:00, ?it/s]\n",
      "Get data via HH API: 0it [00:00, ?it/s]\n",
      "Get data via HH API: 0it [00:00, ?it/s]\n",
      "Get data via HH API: 0it [00:00, ?it/s]\n",
      "Get data via HH API: 0it [00:00, ?it/s]\n",
      "Get data via HH API: 0it [00:00, ?it/s]\n",
      "Get data via HH API: 0it [00:00, ?it/s]\n",
      "Get data via HH API: 0it [00:00, ?it/s]\n",
      "Get data via HH API: 0it [00:00, ?it/s]\n",
      "Get data via HH API: 0it [00:00, ?it/s]\n",
      "Get data via HH API: 0it [00:00, ?it/s]\n",
      "Get data via HH API: 0it [00:00, ?it/s]\n",
      "Get data via HH API: 0it [00:00, ?it/s]\n",
      "Get data via HH API: 0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading for query 'Бизнес аналитик' for 2022-09-29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Get data via HH API: 0it [00:00, ?it/s]\n",
      "Get data via HH API: 0it [00:00, ?it/s]\n",
      "Get data via HH API: 0it [00:00, ?it/s]\n",
      "Get data via HH API: 0it [00:00, ?it/s]\n",
      "Get data via HH API: 0it [00:00, ?it/s]\n",
      "Get data via HH API: 0it [00:00, ?it/s]\n",
      "Get data via HH API: 0it [00:00, ?it/s]\n",
      "Get data via HH API: 0it [00:00, ?it/s]\n",
      "Get data via HH API: 0it [00:00, ?it/s]\n",
      "Get data via HH API: 0it [00:00, ?it/s]\n",
      "Get data via HH API: 0it [00:00, ?it/s]\n",
      "Get data via HH API: 0it [00:00, ?it/s]\n",
      "Get data via HH API: 0it [00:00, ?it/s]\n",
      "Get data via HH API: 0it [00:00, ?it/s]\n",
      "Get data via HH API: 0it [00:00, ?it/s]\n",
      "Get data via HH API: 0it [00:00, ?it/s]\n",
      "Get data via HH API: 0it [00:00, ?it/s]\n",
      "Get data via HH API: 0it [00:00, ?it/s]\n",
      "Get data via HH API: 0it [00:00, ?it/s]\n",
      "Get data via HH API: 0it [00:00, ?it/s]\n",
      "Get data via HH API: 0it [00:00, ?it/s]\n",
      "Get data via HH API: 0it [00:00, ?it/s]\n",
      "Get data via HH API: 0it [00:00, ?it/s]\n",
      "Get data via HH API: 0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading for query 'Продуктовый аналитик' for 2022-09-29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Get data via HH API: 0it [00:00, ?it/s]\n",
      "Get data via HH API: 0it [00:00, ?it/s]\n",
      "Get data via HH API: 0it [00:00, ?it/s]\n",
      "Get data via HH API: 0it [00:00, ?it/s]\n",
      "Get data via HH API: 0it [00:00, ?it/s]\n",
      "Get data via HH API: 0it [00:00, ?it/s]\n",
      "Get data via HH API: 0it [00:00, ?it/s]\n",
      "Get data via HH API: 0it [00:00, ?it/s]\n",
      "Get data via HH API: 0it [00:00, ?it/s]\n",
      "Get data via HH API: 0it [00:00, ?it/s]\n",
      "Get data via HH API: 0it [00:00, ?it/s]\n",
      "Get data via HH API: 0it [00:00, ?it/s]\n",
      "Get data via HH API: 0it [00:00, ?it/s]\n",
      "Get data via HH API: 0it [00:00, ?it/s]\n",
      "Get data via HH API: 0it [00:00, ?it/s]\n",
      "Get data via HH API: 0it [00:00, ?it/s]\n",
      "Get data via HH API: 0it [00:00, ?it/s]\n",
      "Get data via HH API: 0it [00:00, ?it/s]\n",
      "Get data via HH API: 0it [00:00, ?it/s]\n",
      "Get data via HH API: 0it [00:00, ?it/s]\n",
      "Get data via HH API: 0it [00:00, ?it/s]\n",
      "Get data via HH API: 0it [00:00, ?it/s]\n",
      "Get data via HH API: 0it [00:00, ?it/s]\n",
      "Get data via HH API: 100%|████████████████████████████████████████████| 1/1 [00:00<00:00,  8.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading for query 'Marketing Analyst' for 2022-09-29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Get data via HH API: 0it [00:00, ?it/s]\n",
      "Get data via HH API: 0it [00:00, ?it/s]\n",
      "Get data via HH API: 0it [00:00, ?it/s]\n",
      "Get data via HH API: 0it [00:00, ?it/s]\n",
      "Get data via HH API: 0it [00:00, ?it/s]\n",
      "Get data via HH API: 0it [00:00, ?it/s]\n",
      "Get data via HH API: 0it [00:00, ?it/s]\n",
      "Get data via HH API: 0it [00:00, ?it/s]\n",
      "Get data via HH API: 0it [00:00, ?it/s]\n",
      "Get data via HH API: 0it [00:00, ?it/s]\n",
      "Get data via HH API: 0it [00:00, ?it/s]\n",
      "Get data via HH API: 100%|████████████████████████████████████████████| 6/6 [00:00<00:00,  7.82it/s]\n",
      "Get data via HH API: 100%|████████████████████████████████████████████| 3/3 [00:00<00:00,  9.52it/s]\n",
      "Get data via HH API: 100%|████████████████████████████████████████████| 8/8 [00:00<00:00,  8.42it/s]\n",
      "Get data via HH API: 100%|████████████████████████████████████████████| 4/4 [00:00<00:00,  8.16it/s]\n",
      "Get data via HH API: 100%|████████████████████████████████████████████| 5/5 [00:00<00:00,  7.05it/s]\n",
      "Get data via HH API: 100%|████████████████████████████████████████████| 3/3 [00:00<00:00, 10.16it/s]\n",
      "Get data via HH API: 100%|████████████████████████████████████████████| 2/2 [00:00<00:00,  8.44it/s]\n",
      "Get data via HH API: 100%|████████████████████████████████████████████| 3/3 [00:00<00:00,  8.35it/s]\n",
      "Get data via HH API: 100%|████████████████████████████████████████████| 1/1 [00:00<00:00,  6.49it/s]\n",
      "Get data via HH API: 0it [00:00, ?it/s]\n",
      "Get data via HH API: 0it [00:00, ?it/s]\n",
      "Get data via HH API: 100%|████████████████████████████████████████████| 2/2 [00:00<00:00,  8.47it/s]\n",
      "Get data via HH API: 0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading for query 'Веб-аналитик' for 2022-09-29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Get data via HH API: 100%|████████████████████████████████████████████| 1/1 [00:00<00:00,  7.24it/s]\n",
      "Get data via HH API: 100%|████████████████████████████████████████████| 1/1 [00:00<00:00,  9.61it/s]\n",
      "Get data via HH API: 0it [00:00, ?it/s]\n",
      "Get data via HH API: 0it [00:00, ?it/s]\n",
      "Get data via HH API: 100%|████████████████████████████████████████████| 2/2 [00:00<00:00,  9.43it/s]\n",
      "Get data via HH API: 100%|████████████████████████████████████████████| 1/1 [00:00<00:00,  8.84it/s]\n",
      "Get data via HH API: 100%|████████████████████████████████████████████| 5/5 [00:00<00:00,  8.05it/s]\n",
      "Get data via HH API: 100%|██████████████████████████████████████████| 10/10 [00:01<00:00,  7.85it/s]\n",
      "Get data via HH API: 100%|████████████████████████████████████████████| 8/8 [00:00<00:00,  9.24it/s]\n",
      "Get data via HH API: 100%|██████████████████████████████████████████| 25/25 [00:03<00:00,  7.32it/s]\n",
      "Get data via HH API: 100%|██████████████████████████████████████████| 14/14 [00:01<00:00,  8.19it/s]\n",
      "Get data via HH API: 100%|██████████████████████████████████████████| 22/22 [00:02<00:00,  8.66it/s]\n",
      "Get data via HH API: 100%|██████████████████████████████████████████| 21/21 [00:02<00:00,  8.64it/s]\n",
      "Get data via HH API: 100%|██████████████████████████████████████████| 11/11 [00:01<00:00,  7.43it/s]\n",
      "Get data via HH API: 100%|██████████████████████████████████████████| 23/23 [00:02<00:00,  7.89it/s]\n",
      "Get data via HH API: 100%|██████████████████████████████████████████| 12/12 [00:01<00:00,  8.90it/s]\n",
      "Get data via HH API: 100%|██████████████████████████████████████████| 21/21 [00:02<00:00,  7.83it/s]\n",
      "Get data via HH API: 100%|██████████████████████████████████████████| 13/13 [00:02<00:00,  5.40it/s]\n",
      "Get data via HH API: 100%|██████████████████████████████████████████| 10/10 [00:01<00:00,  7.88it/s]\n",
      "Get data via HH API: 100%|████████████████████████████████████████████| 5/5 [00:00<00:00,  8.31it/s]\n",
      "Get data via HH API: 100%|████████████████████████████████████████████| 2/2 [00:00<00:00,  8.09it/s]\n",
      "Get data via HH API: 100%|████████████████████████████████████████████| 3/3 [00:00<00:00,  7.71it/s]\n",
      "Get data via HH API: 100%|████████████████████████████████████████████| 3/3 [00:00<00:00,  9.80it/s]\n",
      "Get data via HH API: 100%|████████████████████████████████████████████| 4/4 [00:00<00:00,  8.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading for query 'Аналитик BI' for 2022-09-29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Get data via HH API: 100%|████████████████████████████████████████████| 2/2 [00:00<00:00,  8.73it/s]\n",
      "Get data via HH API: 0it [00:00, ?it/s]\n",
      "Get data via HH API: 0it [00:00, ?it/s]\n",
      "Get data via HH API: 0it [00:00, ?it/s]\n",
      "Get data via HH API: 0it [00:00, ?it/s]\n",
      "Get data via HH API: 100%|████████████████████████████████████████████| 3/3 [00:00<00:00,  7.81it/s]\n",
      "Get data via HH API: 100%|████████████████████████████████████████████| 2/2 [00:00<00:00,  7.63it/s]\n",
      "Get data via HH API: 100%|████████████████████████████████████████████| 6/6 [00:00<00:00,  8.45it/s]\n",
      "Get data via HH API: 100%|████████████████████████████████████████████| 9/9 [00:01<00:00,  7.01it/s]\n",
      "Get data via HH API: 100%|██████████████████████████████████████████| 17/17 [00:02<00:00,  7.12it/s]\n",
      "Get data via HH API: 100%|██████████████████████████████████████████| 23/23 [00:02<00:00,  7.98it/s]\n",
      "Get data via HH API: 100%|██████████████████████████████████████████| 25/25 [00:03<00:00,  8.28it/s]\n",
      "Get data via HH API: 100%|██████████████████████████████████████████| 15/15 [00:02<00:00,  7.36it/s]\n",
      "Get data via HH API: 100%|██████████████████████████████████████████| 17/17 [00:02<00:00,  7.04it/s]\n",
      "Get data via HH API: 100%|██████████████████████████████████████████| 17/17 [00:02<00:00,  7.40it/s]\n",
      "Get data via HH API: 100%|██████████████████████████████████████████| 27/27 [00:03<00:00,  8.35it/s]\n",
      "Get data via HH API: 100%|██████████████████████████████████████████| 14/14 [00:02<00:00,  6.82it/s]\n",
      "Get data via HH API: 100%|██████████████████████████████████████████| 15/15 [00:02<00:00,  7.12it/s]\n",
      "Get data via HH API: 100%|████████████████████████████████████████████| 8/8 [00:00<00:00,  8.71it/s]\n",
      "Get data via HH API: 100%|████████████████████████████████████████████| 5/5 [00:00<00:00,  7.56it/s]\n",
      "Get data via HH API: 100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 11.62it/s]\n",
      "Get data via HH API: 100%|████████████████████████████████████████████| 4/4 [00:00<00:00,  7.09it/s]\n",
      "Get data via HH API: 100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 10.20it/s]\n",
      "Get data via HH API: 0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading for query 'Младший аналитик' for 2022-09-29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Get data via HH API: 0it [00:00, ?it/s]\n",
      "Get data via HH API: 100%|████████████████████████████████████████████| 1/1 [00:00<00:00,  8.81it/s]\n",
      "Get data via HH API: 100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 11.36it/s]\n",
      "Get data via HH API: 100%|████████████████████████████████████████████| 3/3 [00:00<00:00,  6.49it/s]\n",
      "Get data via HH API: 0it [00:00, ?it/s]\n",
      "Get data via HH API: 100%|████████████████████████████████████████████| 3/3 [00:00<00:00,  7.42it/s]\n",
      "Get data via HH API: 100%|████████████████████████████████████████████| 2/2 [00:00<00:00,  8.36it/s]\n",
      "Get data via HH API: 100%|████████████████████████████████████████████| 6/6 [00:00<00:00, 10.67it/s]\n",
      "Get data via HH API: 100%|██████████████████████████████████████████| 13/13 [00:01<00:00,  8.75it/s]\n",
      "Get data via HH API: 100%|██████████████████████████████████████████| 14/14 [00:01<00:00,  8.16it/s]\n",
      "Get data via HH API: 100%|██████████████████████████████████████████| 20/20 [00:02<00:00,  8.10it/s]\n",
      "Get data via HH API: 100%|██████████████████████████████████████████| 24/24 [00:02<00:00,  8.37it/s]\n",
      "Get data via HH API: 100%|██████████████████████████████████████████| 27/27 [00:03<00:00,  8.49it/s]\n",
      "Get data via HH API: 100%|██████████████████████████████████████████| 23/23 [00:02<00:00,  9.19it/s]\n",
      "Get data via HH API: 100%|██████████████████████████████████████████| 26/26 [00:02<00:00,  9.02it/s]\n",
      "Get data via HH API: 100%|██████████████████████████████████████████| 81/81 [00:08<00:00,  9.08it/s]\n",
      "Get data via HH API: 100%|██████████████████████████████████████████| 28/28 [00:04<00:00,  6.43it/s]\n",
      "Get data via HH API: 100%|██████████████████████████████████████████| 15/15 [00:01<00:00,  8.54it/s]\n",
      "Get data via HH API: 100%|██████████████████████████████████████████| 13/13 [00:01<00:00,  8.40it/s]\n",
      "Get data via HH API: 100%|████████████████████████████████████████████| 7/7 [00:00<00:00,  8.95it/s]\n",
      "Get data via HH API: 100%|████████████████████████████████████████████| 5/5 [00:00<00:00, 10.32it/s]\n",
      "Get data via HH API: 100%|████████████████████████████████████████████| 4/4 [00:00<00:00,  8.98it/s]\n",
      "Get data via HH API: 100%|████████████████████████████████████████████| 2/2 [00:00<00:00,  6.41it/s]\n",
      "Get data via HH API: 0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading for query 'Руководитель отдела аналитики' for 2022-09-29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Get data via HH API: 100%|████████████████████████████████████████████| 2/2 [00:00<00:00, 11.16it/s]\n",
      "Get data via HH API: 100%|████████████████████████████████████████████| 2/2 [00:00<00:00, 10.69it/s]\n",
      "Get data via HH API: 100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 10.41it/s]\n",
      "Get data via HH API: 100%|████████████████████████████████████████████| 2/2 [00:00<00:00,  8.64it/s]\n",
      "Get data via HH API: 100%|████████████████████████████████████████████| 3/3 [00:00<00:00, 10.48it/s]\n",
      "Get data via HH API: 100%|████████████████████████████████████████████| 6/6 [00:00<00:00,  7.95it/s]\n",
      "Get data via HH API: 100%|████████████████████████████████████████████| 2/2 [00:00<00:00,  8.13it/s]\n",
      "Get data via HH API: 100%|████████████████████████████████████████████| 6/6 [00:00<00:00,  8.76it/s]\n",
      "Get data via HH API: 100%|██████████████████████████████████████████| 20/20 [00:02<00:00,  7.79it/s]\n",
      "Get data via HH API: 100%|██████████████████████████████████████████| 49/49 [00:06<00:00,  8.14it/s]\n",
      "Get data via HH API: 100%|██████████████████████████████████████████| 45/45 [00:05<00:00,  8.34it/s]\n",
      "Get data via HH API: 100%|██████████████████████████████████████████| 35/35 [00:04<00:00,  7.82it/s]\n",
      "Get data via HH API: 100%|██████████████████████████████████████████| 40/40 [00:05<00:00,  7.65it/s]\n",
      "Get data via HH API: 100%|██████████████████████████████████████████| 31/31 [00:03<00:00,  8.03it/s]\n",
      "Get data via HH API: 100%|██████████████████████████████████████████| 22/22 [00:02<00:00,  8.58it/s]\n",
      "Get data via HH API: 100%|██████████████████████████████████████████| 38/38 [00:04<00:00,  8.04it/s]\n",
      "Get data via HH API: 100%|██████████████████████████████████████████| 29/29 [00:03<00:00,  8.47it/s]\n",
      "Get data via HH API: 100%|██████████████████████████████████████████| 24/24 [00:02<00:00,  8.08it/s]\n",
      "Get data via HH API: 100%|██████████████████████████████████████████| 12/12 [00:01<00:00,  9.19it/s]\n",
      "Get data via HH API: 100%|██████████████████████████████████████████| 12/12 [00:01<00:00,  9.12it/s]\n",
      "Get data via HH API: 100%|████████████████████████████████████████████| 7/7 [00:01<00:00,  6.33it/s]\n",
      "Get data via HH API: 100%|████████████████████████████████████████████| 4/4 [00:00<00:00,  8.63it/s]\n",
      "Get data via HH API: 100%|████████████████████████████████████████████| 6/6 [00:00<00:00,  8.83it/s]\n",
      "Get data via HH API: 100%|████████████████████████████████████████████| 4/4 [00:00<00:00,  7.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading for query 'Аналитик баз данных' for 2022-09-29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Get data via HH API: 0it [00:00, ?it/s]\n",
      "Get data via HH API: 100%|████████████████████████████████████████████| 1/1 [00:00<00:00,  7.87it/s]\n",
      "Get data via HH API: 100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 10.86it/s]\n",
      "Get data via HH API: 0it [00:00, ?it/s]\n",
      "Get data via HH API: 0it [00:00, ?it/s]\n",
      "Get data via HH API: 0it [00:00, ?it/s]\n",
      "Get data via HH API: 100%|████████████████████████████████████████████| 1/1 [00:00<00:00,  8.77it/s]\n",
      "Get data via HH API: 100%|████████████████████████████████████████████| 2/2 [00:00<00:00,  9.95it/s]\n",
      "Get data via HH API: 100%|████████████████████████████████████████████| 4/4 [00:00<00:00,  8.28it/s]\n",
      "Get data via HH API: 100%|████████████████████████████████████████████| 9/9 [00:00<00:00,  9.12it/s]\n",
      "Get data via HH API: 100%|██████████████████████████████████████████| 11/11 [00:01<00:00,  8.17it/s]\n",
      "Get data via HH API: 100%|████████████████████████████████████████████| 7/7 [00:00<00:00,  8.04it/s]\n",
      "Get data via HH API: 100%|████████████████████████████████████████████| 7/7 [00:00<00:00,  8.45it/s]\n",
      "Get data via HH API: 100%|████████████████████████████████████████████| 6/6 [00:00<00:00, 10.48it/s]\n",
      "Get data via HH API: 100%|██████████████████████████████████████████| 12/12 [00:01<00:00,  7.40it/s]\n",
      "Get data via HH API: 100%|████████████████████████████████████████████| 6/6 [00:00<00:00,  6.26it/s]\n",
      "Get data via HH API: 100%|████████████████████████████████████████████| 7/7 [00:00<00:00,  7.47it/s]\n",
      "Get data via HH API: 100%|████████████████████████████████████████████| 4/4 [00:00<00:00,  8.18it/s]\n",
      "Get data via HH API: 100%|██████████████████████████████████████████| 10/10 [00:01<00:00,  8.13it/s]\n",
      "Get data via HH API: 100%|████████████████████████████████████████████| 3/3 [00:00<00:00, 10.67it/s]\n",
      "Get data via HH API: 100%|████████████████████████████████████████████| 1/1 [00:00<00:00,  8.10it/s]\n",
      "Get data via HH API: 100%|████████████████████████████████████████████| 2/2 [00:00<00:00,  7.60it/s]\n",
      "Get data via HH API: 0it [00:00, ?it/s]\n",
      "Get data via HH API: 0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading for query 'Инженер баз данных' for 2022-09-29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Get data via HH API: 0it [00:00, ?it/s]\n",
      "Get data via HH API: 100%|████████████████████████████████████████████| 2/2 [00:00<00:00, 10.31it/s]\n",
      "Get data via HH API: 100%|████████████████████████████████████████████| 1/1 [00:00<00:00,  8.19it/s]\n",
      "Get data via HH API: 0it [00:00, ?it/s]\n",
      "Get data via HH API: 0it [00:00, ?it/s]\n",
      "Get data via HH API: 0it [00:00, ?it/s]\n",
      "Get data via HH API: 100%|████████████████████████████████████████████| 1/1 [00:00<00:00,  3.18it/s]\n",
      "Get data via HH API: 0it [00:00, ?it/s]\n",
      "Get data via HH API: 100%|████████████████████████████████████████████| 3/3 [00:00<00:00,  9.37it/s]\n",
      "Get data via HH API: 100%|████████████████████████████████████████████| 7/7 [00:00<00:00,  8.31it/s]\n",
      "Get data via HH API: 100%|████████████████████████████████████████████| 7/7 [00:00<00:00,  8.37it/s]\n",
      "Get data via HH API: 100%|██████████████████████████████████████████| 10/10 [00:01<00:00,  6.87it/s]\n",
      "Get data via HH API: 100%|████████████████████████████████████████████| 8/8 [00:00<00:00,  9.03it/s]\n",
      "Get data via HH API: 100%|████████████████████████████████████████████| 5/5 [00:00<00:00,  7.44it/s]\n",
      "Get data via HH API: 100%|████████████████████████████████████████████| 7/7 [00:00<00:00,  8.76it/s]\n",
      "Get data via HH API: 100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 11.62it/s]\n",
      "Get data via HH API: 100%|████████████████████████████████████████████| 9/9 [00:01<00:00,  8.47it/s]\n",
      "Get data via HH API: 100%|████████████████████████████████████████████| 2/2 [00:00<00:00,  8.54it/s]\n",
      "Get data via HH API: 0it [00:00, ?it/s]\n",
      "Get data via HH API: 100%|████████████████████████████████████████████| 1/1 [00:00<00:00,  5.31it/s]\n",
      "Get data via HH API: 100%|████████████████████████████████████████████| 2/2 [00:00<00:00,  7.78it/s]\n",
      "Get data via HH API: 100%|████████████████████████████████████████████| 3/3 [00:00<00:00,  8.69it/s]\n",
      "Get data via HH API: 0it [00:00, ?it/s]\n",
      "Get data via HH API: 0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading for query 'Data Engineer' for 2022-09-29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Get data via HH API: 0it [00:00, ?it/s]\n",
      "Get data via HH API: 0it [00:00, ?it/s]\n",
      "Get data via HH API: 0it [00:00, ?it/s]\n",
      "Get data via HH API: 100%|████████████████████████████████████████████| 1/1 [00:00<00:00,  8.12it/s]\n",
      "Get data via HH API: 0it [00:00, ?it/s]\n",
      "Get data via HH API: 0it [00:00, ?it/s]\n",
      "Get data via HH API: 100%|████████████████████████████████████████████| 2/2 [00:00<00:00,  5.58it/s]\n",
      "Get data via HH API: 100%|████████████████████████████████████████████| 1/1 [00:00<00:00,  9.90it/s]\n",
      "Get data via HH API: 100%|██████████████████████████████████████████| 10/10 [00:01<00:00,  8.13it/s]\n",
      "Get data via HH API: 100%|██████████████████████████████████████████| 11/11 [00:01<00:00,  6.92it/s]\n",
      "Get data via HH API: 100%|██████████████████████████████████████████| 15/15 [00:02<00:00,  7.26it/s]\n",
      "Get data via HH API: 100%|██████████████████████████████████████████| 12/12 [00:01<00:00,  7.31it/s]\n",
      "Get data via HH API: 100%|████████████████████████████████████████████| 9/9 [00:01<00:00,  7.43it/s]\n",
      "Get data via HH API: 100%|██████████████████████████████████████████| 25/25 [00:03<00:00,  8.15it/s]\n",
      "Get data via HH API: 100%|██████████████████████████████████████████| 15/15 [00:01<00:00,  7.71it/s]\n",
      "Get data via HH API: 100%|████████████████████████████████████████████| 8/8 [00:01<00:00,  7.19it/s]\n",
      "Get data via HH API: 100%|████████████████████████████████████████████| 9/9 [00:01<00:00,  7.71it/s]\n",
      "Get data via HH API: 100%|████████████████████████████████████████████| 8/8 [00:01<00:00,  6.73it/s]\n",
      "Get data via HH API: 100%|████████████████████████████████████████████| 5/5 [00:00<00:00,  8.23it/s]\n",
      "Get data via HH API: 100%|████████████████████████████████████████████| 1/1 [00:00<00:00,  7.19it/s]\n",
      "Get data via HH API: 100%|████████████████████████████████████████████| 3/3 [00:00<00:00,  8.57it/s]\n",
      "Get data via HH API: 100%|████████████████████████████████████████████| 4/4 [00:00<00:00,  9.90it/s]\n",
      "Get data via HH API: 100%|████████████████████████████████████████████| 2/2 [00:00<00:00,  9.66it/s]\n",
      "Get data via HH API: 0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading for query 'Data Scientist' for 2022-09-29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Get data via HH API: 0it [00:00, ?it/s]\n",
      "Get data via HH API: 0it [00:00, ?it/s]\n",
      "Get data via HH API: 0it [00:00, ?it/s]\n",
      "Get data via HH API: 0it [00:00, ?it/s]\n",
      "Get data via HH API: 0it [00:00, ?it/s]\n",
      "Get data via HH API: 0it [00:00, ?it/s]\n",
      "Get data via HH API: 0it [00:00, ?it/s]\n",
      "Get data via HH API: 0it [00:00, ?it/s]\n",
      "Get data via HH API: 100%|████████████████████████████████████████████| 2/2 [00:00<00:00,  9.47it/s]\n",
      "Get data via HH API: 0it [00:00, ?it/s]\n",
      "Get data via HH API: 100%|████████████████████████████████████████████| 8/8 [00:00<00:00,  8.58it/s]\n",
      "Get data via HH API: 100%|████████████████████████████████████████████| 5/5 [00:00<00:00,  6.43it/s]\n",
      "Get data via HH API: 100%|████████████████████████████████████████████| 5/5 [00:00<00:00,  8.59it/s]\n",
      "Get data via HH API: 100%|████████████████████████████████████████████| 2/2 [00:00<00:00,  6.49it/s]\n",
      "Get data via HH API: 100%|████████████████████████████████████████████| 1/1 [00:00<00:00,  9.70it/s]\n",
      "Get data via HH API: 100%|████████████████████████████████████████████| 4/4 [00:00<00:00,  8.13it/s]\n",
      "Get data via HH API: 100%|████████████████████████████████████████████| 1/1 [00:00<00:00,  9.70it/s]\n",
      "Get data via HH API: 0it [00:00, ?it/s]\n",
      "Get data via HH API: 0it [00:00, ?it/s]\n",
      "Get data via HH API: 100%|████████████████████████████████████████████| 2/2 [00:00<00:00,  8.81it/s]\n",
      "Get data via HH API: 0it [00:00, ?it/s]\n",
      "Get data via HH API: 100%|████████████████████████████████████████████| 1/1 [00:00<00:00,  7.19it/s]\n",
      "Get data via HH API: 100%|████████████████████████████████████████████| 1/1 [00:00<00:00,  8.26it/s]\n",
      "Get data via HH API: 0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded all 2171 vacancies for 2022-09-29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# '2022-09-22'\n",
    "retrieve_queries(query_texts2, (date.today() - td(days = 1)).isoformat(), refresh=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
